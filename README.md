# ğŸ§Š AWS ETL Pipeline: S3 âœ Lambda âœ Cleaned CSV âœ S3

This project demonstrates an end-to-end **AWS serverless ETL pipeline** using **S3** and **Lambda**. It reads raw employee data, processes it to handle bad records, groups the data by department, and writes the result back to S3 in clean CSV format.

---

## ğŸ“Œ Problem Statement

The input data (`employee_raw.csv`) contains inconsistent or bad entries such as:
- Missing or incorrect `PhoneNumber`
- Invalid `Salary` or `Age` values
- Inconsistent `Dept` or `Designation` fields

The goal is to:
- Clean and validate the data
- Group by department (`Dept`)
- Summarize department-wise employee counts and average salary
- Output a structured, clean CSV to another S3 location

---

## ğŸ§© Sample Raw Data (employee_raw.csv)

| EmpID | Dept | EmpName | Salary | Age | PhoneNumber | Address | JobRole | Designation |
|-------|------|---------|--------|-----|-------------|---------|---------|-------------|
| 101 | Sales | John D | 50000 | 29 | 9876543210 | NYC | Executive | Sales Rep |
| 102 | HR | Jane | -2000 | abc | 1234 | Boston | HR Analyst | HR |
| 103 | Dev | Max | 65000 | 26 | 9000011122 | SF | Developer | SDE I |

---

## âš™ï¸ AWS Services Used

- **Amazon S3** â€“ Source and destination storage for CSV files
- **AWS Lambda** â€“ ETL logic implemented in Python
- **IAM** â€“ Role-based access to S3 from Lambda
- *(Optional)* CloudWatch for logging and monitoring

---

## ğŸš€ Pipeline Workflow

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Raw CSV in â”‚ â”€â”€â”€â”€â”€â–¶ â”‚ Lambda (ETL) â”‚ â”€â”€â”€â”€â–¶ â”‚ Cleaned CSV Output â”‚
â”‚ S3 Bucket â”‚ â”‚ Python â”‚ â”‚ in S3 Output â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
